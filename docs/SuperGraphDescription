### Description

Here the superGraph can combine all lexical graphs,
co-occurence graph and relatedness graphs into one single representation,
namelt a super graph that is an ensemble of all the aforementioned.

### Purpose
Have a versatile and strong representation of documents using all available approaches.

### Applications
* **Classification** - Random weights are assigned to each node in the graph for each feature
(feature being a count measure from each subgraph) to before curating a single valued representation
for each node. The weights can then be tuned, possibly using backprop and gradient descent if neural
networks are being used.
These weights signify the strength of the relationship between any links in the graph. The idea is
that features with string connections would become high when weights are tuned and links are weak between connections
that are weakly connected.

* **Retrieval** - Queries can be made to find the most relevant document
based on the node and link measures in the supergraph. It also means that similarity
between query and document terms since you have all lexical net sources available
and also the co-occrence and relatedness information.

* **Word Vectors** - Word vectors are often approached by using dimensionality reduction
techniques via decompositions (SVD, PCA) or gradient based learning (e.g autoencoders)
Here we can use a graphical vector representation of a word. Here a context vector is
created by searching a term and its surrounding terms in each document graph.
Each document graph returns a score for the relation between the word of interest w
and the neighbouring words in the graphs.
This leads to a 3d tensor where r is documents, columns c are graph terms
and k is the graph score for each document graph for a particular term in c.
This 3d tensor can then be passed as to a denoising autoencoder to produce a
2d matrix document-term matrix.

